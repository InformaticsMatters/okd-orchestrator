---

# --------------------------------------------------------------------------- #
# An orchestrator deployment configuration file.                              #
#                                                                             #
# ...compatible with orchestrator configuration file format version 1.        #
#                                                                             #
# This working configuration can also act as a template for your own          #
# deployment. If the 'compact' style suits your needs then feel free          #
# to copy this to form the basic of your own deployment.                      #
#                                                                             #
# To avoid the comments from becoming out-of-date you should probably strip   #
# the documentation that accompanies this file from your copy, You can        #
# rest-assured that this Frankfurt file will be kept up to date               #
# with any orchestrator changes and new features.                             #
# --------------------------------------------------------------------------- #

# The configuration file version. This is dictated by the orchestrator.
# As enhanced file versions are brought online this number will
# increase. Whether your configuration version is supported is a function of
# the orchestrator version you are running.
version: 1

# The name of the deployment. and description of the configuration.
#  This is used for reporting purposes only.
name: Compact AWS Deployment (OKD 3.11) [Frankfurt]
# A more detailed description of the deployment.
# This is used for reporting purposes only.
description: >
  A combined Bastion/Master/Infra/Compute deployment  where a single
  cloud instance acts as a compute node and the bastion, master and
  infrastructure node.
  This example also includes certbot certificate creation,
  metrics and prometheus.

# Details of the OKD cluster configuration.
# As well as the cluster region (provider-specific)
# and its hostname the size and machine details of the OKD
# cloud instances are defined.
cluster:

  # The provider 'region' used for cluster instances.
  # The value is provider-specific.
  region: eu-central-1
  # A unique name for your cluster.
  # All your clusters must have a different ID.
  id: compact-frankfurt-311
  # Machine image name for the cluster instances.
  # This should match the start of the Packer-generated images.
  # The latest image whose name starts with this will be used.
  # The Bastion, if used, uses its own image.
  image_prefix: okd-machine-image-311-

  # A name prefix for the names of each cluster object instance.
  # Where supported by your provider this value will be used to alter the
  # instance names of objects created in your cloud provider.
  # Nodes will be named 'node-NNN-<nodes-name>' but, if your prefix
  # is 'vre', nodes named 'compute' will be named 'vre-node-001-general'.
  # Use this to distinguish between nodes of different clusters in the
  # same cloud provider.
  name_tag: compact
  # The external IP address or domain-name of the Master node.
  # If not set the master public DNS will be used.
  # Whatever is used must provide a resolvable route to the master.
  # So, if you have a domain, ensure it's routed to the pre-allocated
  # public IP of the master (see cluster.master.fixed_ip_id).
  # If you use an external load balancer, you should specify the address of
  # the external load balancer.
  public_hostname: compact.informaticsmatters.com
  # The external IP address or domain-name for application default routes.
  # This is a suffix appended to application $service.$namespace routes
  # for those that need a default. It should resolve to the Infrastructure
  # (Router) node.
  default_subdomain: compact.informaticsmatters.com
  # The Master API/Console Port
  api_port: 8443

  # Configuration details of the Bastion instance.
  # This section is ignored if the cluster.master is
  # configured to also act as a Bastion
  # (i.e. cluster.master.is_bastion resolves to true)
  bastion:
    # Machine image name for the Bastion instance.
    # This should match the start of the Packer-generated image.
    # The latest image whose name starts with this will be used.
    # Not required if the Master is acting as a Bastion.
    image_prefix: okd-bastion-image-311-
    # Provider-specific machine image type.
    # See master.type.
    # Not required if the Master is acting as a Bastion.
    type: t2.small

  # Details of the Master nodes.
  #
  # 'Simple' deployment topologies (as this is)
  # are suitable when we're using just 1 Master instance.
  master:
    count: 1
    # Provider-specific machine image type.
    # i.e. t2.xlarge on AWS or START1-L on Scaleway.
    type: t2.xlarge
    # If you have pre-allocated public IPs for the Master set the value here.
    # This is _not_ normally the IP address itself, it is the cloud-provider's
    # IP identity. In AWS this would be the Elastic IP Allocation ID
    # e.g. "eipalloc-0000000000000000".
    #
    # If you have not provided a pre-allocated (fixed) IP
    # leave this property empty.
    fixed_ip_id: eipalloc-010cfe12a8b8c7fa2
    # If the master is to also act as the infrastructure node
    # then set this field to 'yes' while also setting the 'infra.count' to 0.
    is_infra: yes
    # If the master is to also act as the Bastion node
    # then set this field to 'yes'.
    is_bastion: yes
    # Set if you also want the Master nod eto be a general compute node.
    # You can omit this property.
    # You can also make the node a general compute node after it's been created
    # by using with the 'oc label' command: -
    #     oc label node <node> node-role.kubernetes.io/compute=true
    is_compute_node: yes

  # Details of the Infrastructure nodes.
  infra:
    # Provider-specific machine image type.
    # Not required (and ignored) if the Master is acting as a Infrastructure.
    count: 0
    # Provider-specific machine image type.
    # Not required (and ignored) if the Master is acting as a Infrastructure.
#    type:
    # The (optional) pre-allocated public IP for the Infrastructure.
    # (See cluster.master.fixed_ip).
    # Not required (and ignored) if the Master is acting as a Infrastructure.
#    fixed_ip_id:
    # Set if you also want the Infrastructure nodes to act as general compute node.
    # See master.is_compute_node
    # Not required (and ignored) if the Master is acting as a Infrastructure.
#    is_compute_node: no

  # You can define extra node groups that your nodes can be assigned to.
  # These complement the expected/built-in groups
  # (e.g. master, infra, compute).
  #
  # For each group you define a 'node_group_<name>' will be created.
  #
  # Node groups consist of: -
  #
  # - An arbitrary and unique 'name' (following standard variable conventions).
  #   Stick to letters, numbers and '_'
  # - A 'label' string that's essentially a 'keyword=value' pair.
  #   The label is used for Pod node selection.
  # - An 'is_compute_node' flag. If this evaluates to true
  #   the node is also treated by OKD as a generally available
  #   compute node where Pods can be scheduled using default rules.
  #   If you need an 'exclusive node' (one that is only used for jobs
  #   that name the label explicitly for selection) then set this to false.
  #   By default this is 'no' so groups are not general compute nodes.
  #
  # You must not use the following names,
  # which are built-in and created automatically during the installation: -
  #
  # - master
  # - master-compute
  # - infra
  # - infra-compute
  # - compute
  # - master-infra
  # - all-in-one
  #
  # You can leave this list blank if you do not need any of your own
  # node groups.
# node_groups:
# - name: general
#   label: general-node=true
#   is_compute_node: yes

  # Details of the cluster compute nodes.
  # This is a list that allows you to define different node 'categories'.
  # Each entry in the list supports the the following properties:
  #
  # Where the provider allows, nodes are placed in different 'zones'.
  # AWS typically provides 3 'zones' per region. So, if you create
  # 6 nodes in a group then two will be placed in 'zone a', two in 'zone b'
  # and two in 'zone c'. Each new node in a group is placed in a different
  # zone with the first node in each group being placed in the first zone.
  #
  # You must have at least 1 compute node.
  #
  # Node consist of: -
  #
  # - A 'name' used simply to tag the resource, for providers that support it.
  # - A numerical 'count' to define the number of nodes of this category to
  #   create in the cluster. It can be zero if you do not want any
  #   independent node groups.
  # - A provider-specific 'type' used to define the compute instance type
  #   for each node in the category. In AWS this might be 't2.medium'.
  # - A 'node_group', used to identify the OKD node group the nodes of this
  #   category belong to. OKD provides built-in groups like 'compute'
  #   for general compute nodes but you can define your own groups
  #   (see the node_groups section above) and place your node into one of
  #   those groups. By convention groups are called 'node-config-<name>'
  #   (like 'node-config-compute'). Here you simply need the name
  #   (i.e 'compute'). If you want to place your node into a build-in
  #   group, you can use that group (i.e. 'compute').
  nodes:
  - count: 0
#   name: general
#   type: t2.medium
#   node_group: general

# The okd section provides details of the OpenShift/OKD deployment.
# These are high-level, abstract, configuration details,
# that include the tagged version of the software to install
# and the playbooks to run.
okd:

  # The version tag for OpenShift Origin/OKD ansible playbooks.
  # This must be a valid tag in the OpenShift git repository.
  ansible_tag: openshift-ansible-3.11.58-1
  # The version of Ansible required to deploy the cluster.
  # The orchestrator will ensure that the correct version of
  # ansible is installed. v2.7.5 is likely to be suitable for
  # OKD 3.11 and beyond but change it here if you need to.
  ansible_version: 2.7.5
  # The names of the playbooks to play in order to deploy the OKD software.
  # This, for 3.11, is typically 'prerequisites' followed by
  # 'deploy_cluster'. You're unlikely to need to change these.
  play:
  - prerequisites
  - deploy_cluster

  # Install extra stuff based on playbooks in the 'ansible/post-okd' directory.
  # These playbooks will be run on the 'cli-node', a node with a suitable
  # corresponding 'oc' command-line utility (typically the Master).
  # The named playbooks are executed after the OKD software has been installed.
  #
  # For each 'play' a corresponding
  # 'ansible/post-okd/playbooks/<play>/deploy.yaml' must exist.
  post_okd_play:
  - acme-controller

  # What additional OpenShift/OKD services should be enabled?
  # Add services to this list like metrics with something like:
  #
  #   enable:
  #   - metrics
  #
  # We currently support:
  # - metrics
  # - prometheus
  # - tsb (The Template Service Broker)
  # - asb (The Ansible Service Broker)
  enable:
  - metrics
  - prometheus

  # Certificate definitions,
  # currently just the Master API certificate.
  certificates:
    # Set to automatically generate a Let's Encrypt/certbot certificate
    # for the master node when the cluster has been created but prior to
    # OpenShift installation.
    generate_api_cert: no

  # The directory of the OKD Ansible inventory for this deployment.
  # This directory is expected to exist in `okd/inventories'.
  # The inventory directory contains template files for the Bastion
  # and OKD cluster. The templates for a given provider are often
  # shared between similar deployments.
  inventory_dir: simple-aws-3-11
  # The Terraform directory that contains the Terraform definitions
  # that suitable to build the OKD Cluster machines cloud infrastructure.
  terraform_dir: aws
  # You can declare whether the Terraform state files are
  # stored in a globally available shared service. At the moment we support
  # Amazon S3. Leaving this field blank means that terraform state
  # is stored locally (within the project's Terraform directory).
  #
  # An S3 shared-state configuration would look something like:
  #
  #   terraform_shared_state:
  #     s3:
  #       bucket: <s3-bucket-name>
  #       key: <s3-key>
  #       region: <s3-region>
  #       table: <dynamo-table-name>
# terraform_shared_state:
